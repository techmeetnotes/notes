## -- title:       Data as code
## -- library:     ideas
## -- identifier:  19817cd9
## -- format:      commonmark
## -- timestamp:   2021-09-11 20:19:13


## Context

During a live discussion with Lion Kimbro on his Discord server, we've touched the topic of data serialization formats, especially his own in-house format called nLSD (number, lists, strings, dictionaries) (see [here](https://web.archive.org/web/20041014020231/http://onebigsoup.wiki.taoriver.net/moin.cgi/nLSDgraphs) for reference).

Also, related to the same topic I've written another document [Entities, links, data](https://scratchpad.volution.ro/ciprian/992c7f2944456f18cdde77f683f49aa7/1ab930b7.html) that describes the concept of entities, links and data, concepts that I rely upon in this document.

For referring to this document, please use the following URL:
https://scratchpad.volution.ro/ciprian/992c7f2944456f18cdde77f683f49aa7/19817cd9.html




## Code as data

Most would remember the "code as data" concept from the Lisp / Scheme world, one that is central to these languages' power.  In a few words, it says that code or AST (abstract syntax tree) should be easily represented as native data-types, thus allowing one to manipulate the code within the same language.  (This is different from "reflection" that some other languages implement.)

As a matter of fact, in Lisp / Scheme, the syntax for the code is identical with the syntax for the data.  For more details, see [Wikipedia](https://en.wikipedia.org/wiki/Homoiconicity).




## Serialization formats

What are serialization formats actually?  (For example JSON, ASN.1, Protocol Buffers, etc.)

The most straight-forward answer would be:  specifications / protocols on how to interpret streams of bytes as data constructs.

The other answer would be:  algorithms that take streams of bytes and output data constructs, plus the reverse operation.

However, I think there is one more valid answer:  DSL (domain specific languages) that when run by their interpreter build data constructs.

To demonstrate that point, let's take as example a subset of JSON, one that allows only lists and strings (that contain only letters), i.e. `["a", ["b"], []]`.  What if we define a "byte-code interpreter" that supports the following instructions:
* the interpreter has two memory areas, one for the program to be executed, one for the data structures to be constructed;
* the interpreter understands instructions made up of one byte, optionally followed by an arbitrary number of bytes as required by the internal implementation of that instruction;
* these are the understood instructions:
  * `[` -- pushes on the stack a new list;
  * `,` -- pops from the stack the current value, and appends it to the list that now should be at the top of the stack;  (else error and halt);
  * `]` -- the same as `,`;
  * `"` -- if the top of the stack is not a string, pushes on the stack a new string, else do nothing;
  * `a..z` -- (i.e. 26 instructions) appends character (`a` to `z`) to the string that should be at the top of the stack;  (else error and halt);
  * `" "` -- (i.e. space) do nothing;
  * `"ctrl+d"` (i.e. end-of-file byte) -- pops from the stack the current value, which is the result of running the program;  (if the stack is empty, or has more than two values, error and halt;)
* (granted, this isn't quite JSON, because the following `"a"b"`, `"ab` and `"ab"` are all equivalent in generating the `ab` string;  also `[` would yield an empty list, etc.;)




## Data as code

After seeing Lion's nLSD serialization format (see [here](https://web.archive.org/web/20041014020231/http://onebigsoup.wiki.taoriver.net/moin.cgi/nLSDgraphs)), and thinking a little about it, I wondered why can't we have a serialization format that is actually code that when executed would construct the data structures described as such.  (Sort of like how the ADN is the code that if executed by the cell machinery it produces in the end a new cell.)

As a side-note, we can say that any valid program is actually such a serialization format.  For example, the following Python code constructs a list of two elements: `l = list(); l.append(1); l.append(2)`.  However, this is too-much;  we need a simpler DSL that is tasked with just constructing simpler data structures.

I'll describe a first proposal in the next sections, but for now I want to extend a little upon what this would enable.




## Derived data use-case

If our serialization format also allows simple processing, say just simple manipulation (aggregation, projection, etc.) over some initial data (that itself is constructed by the program), coupled with a way to "call another code" (like for example including another file and executing it), we can easily have many views of the same data.

For example, say there is a large dataset (1 GiB) of various data-points.  If one often needs to take this original dataset and massage it in various other structures, then these derived datasets would just use storage without providing nothing unique (that can't be obtained from the original dataset).

Granted, deriving data at deserialization time vs deriving data at serialization time, is a trade off between CPU and memory (disk);  but one can always employ caching to obtain the best of both worlds.




## Copyrighted data use-case

Say there is a useful scientific dataset, but it is copyrighted and licensed under a non-free license, especially one that doesn't allow publishing derived datasets.

Say the original format is not that nice to work with;  say one wants to apply some basic transformations to make it compatible with some standard structure.  (For example, the original data is stored column-by-column, and one wants to transpose that to row-by-row, plus removing some non-useful columns.)

Then legally the derived dataset can't be published because it would infringe the author's copyright and the original license wouldn't permit it.

However, I believe (and this is not a legal advice) that if one would just publish some code to transform it (without containing any embedded data), then that would be just fine.

Imagine now that we have our "data as code" serialization format, that also supports including other "data" from an URL;  imagine the original dataset uses such a serialization format;  then one can just publish the "code" that transforms and the URL where to load the raw dataset from.




## Security considerations

(Please note that wherever I say "code", I don't mean arbitrary code;  I mean a set of well-defined instructions.)

Security is a real concern...  Serialization formats shouldn't allow random code to be executed, because inputs can be controlled by malicious actors.

When I load a JSON file, I don't expect it to take the CPU in an infinite loop, neither do I expect it to trigger the loading of random URL's on the internet.

But we don't live in a perfect world...  Search on the internet for "JSON parser remote code execution", or "XML ...", or "JPEG ...", etc.  It seems parsers are quite susceptible to attacks.

However, these are "bugs", as opposed to what I propose that would enable running code by design.  (Again, code means well-defined instructions.)

On the other hand, implementations should be able to allow the developer to disable certain features (like remote URL loading, etc.), or limit the amount of used resources (like CPU or memory).  (In fact, I think any security conscious deserialization library should provide the developer with configuration options that limit the behavior of the deserializer.)




## Constraints (anti-features)

I think the following should not be allowed by such a "data as code" serialization format:
* non-deterministic behavior -- if one loads the same data (ignoring the remote loading capability), then one should always obtain the same data-structure;
* self-referential data / recursive data-structures -- there are languages (for example Erlang) that just don't support these, and there are languages (for example Rust or C++) that use reference counted heap allocated objects (behind "smart pointers"), that if one constructs recursive data-structures would just create memory-leaks (if one doesn't explicitly break the cycle);

However, for the last item, recursive data-structures, I think there is a more elegant solution, namely entities and links.  (To be described in a future document.)




## UD1 (Universal Data 1) -- brainstorming

Here is an initial try at providing an example of a simple, almost human-readable, serialization format that would allow to construct some basic JSON-like data-structures.

Overall syntax:
* each line represents an instruction and its arguments;
* each line is composed of tokens, separated by one or more white-spaces;
* each token is either a number, boolean, or string as described by the JSON syntax;  (and perhaps `null` as a token?)
* `#` starts a comment that spans the rest of the line;

Overall mechanism:
* the interpreter works with the same values (primitives and data structures) as supported by JSON (i.e. lists, maps, numbers, strings, booleans, `null`);
* there is a stack that can hold any value;
* there is are a finite number of slots that can hold any value, say at most 64K;
* both the stack and the slots can be implemented by two using arrays;

In what follows the instructions are named as such:
* if it starts with `L*` it means it operates on the current list on the stack;
* if it starts with `T*` it operates directly on the stack;
* if it starts with `S*` it operates directly on the slots;
* most operations have both a stack and slot flavor;  (i.e. `LTP` pop the stack then push to list, or `LSP` use the slot then push to list;)
* if it ends with `*TP` it means it pops the stack and does something with that value;

For example, the following code would yield `[5, "abc"]`:
~~~~
LB        # begin a list
TP 5      # push the `5` constant on the stack
LTP       # pop the stack, push that value to the list at the top of the stack
TP "abc"  # push the `"abc"` constant on the stack
LTP
LE        # mark the list at the top of the stack as immutable
RTP       # pop the stack, save that value as the result of the code, halt
~~~~

Alternatively, the following code would yield `[["abc"], ["abc"], ["abc"]]`:
~~~~
LB        # begin the list
TP "abc"  # push the `"abc"` constant on the stack
LTP       # pop she stack, push to the list
LE
STP 1     # pop the stack, save that value to the slot `1`
LB
LSP 1     # access the slot `1`, push that value to the list at the top of the stack
LSP 1     # push it another time
LSP 1     # push it another time
LE
RTP       # pop the stack, return, halt
~~~~

This is not a complete specification of such a language.  It is meant only to give a look-and-feel of what I'm proposing.




## Observations

Looking at the above code it's not very human-readable, at least not when compared to JSON or XML.  However, when compared to any of the binary formats its quite better.

The language can seem quite verbose when compared to JSON, less so when compared to XML.  However, imagine having a JSON array with 10K maps, all having the same 4 fields, two of which have a limited set of values (say strings as enums).  All of a sudden, having access to slots, one can store the field name and enum variants, and just reuse them.

If we strive for efficiency, then we can always designate a binary byte-code representation that would use one byte per instruction, no spaces, etc.
